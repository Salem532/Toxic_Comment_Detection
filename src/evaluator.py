import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
from tqdm import tqdm
import numpy as np
import os
from src.data import TextTokenizer, ToxicDataset
from src.models.lstm import LSTMClassifier

class ToxicEvaluator:
    """æ¶æ„è¯„è®ºæ£€æµ‹æ¨¡å‹è¯„ä¼°å™¨ï¼šå°è£…å®Œæ•´è¯„ä¼°æµç¨‹ï¼Œå¯å¤ç”¨"""
    def __init__(self, config, device):
        self.config = config  
        self.device = device  
        self.label_cols = ["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"]  

    def load_and_clean_test_data(self):
        """åŠ è½½å¹¶æ¸…æ´—æµ‹è¯•é›†æ•°æ®"""
        print("-> æ­£åœ¨å¤„ç†æµ‹è¯•é›†æ•°æ®...")
        test_path = self.config['data']['test_path']
        test_labels_path = self.config['data']['test_labels_path']
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(test_path) or not os.path.exists(test_labels_path):
            raise FileNotFoundError("é”™è¯¯ï¼šæ‰¾ä¸åˆ° test.csv æˆ– test_labels.csv")

        # åŠ è½½å¹¶åˆå¹¶æ–‡æœ¬ä¸æ ‡ç­¾ï¼ˆé€šè¿‡idå¯¹é½ï¼‰
        df_test = pd.read_csv(test_path)
        df_labels = pd.read_csv(test_labels_path)
        test_data = pd.merge(df_test, df_labels, on='id')
        
        # å‰”é™¤æ ‡ç­¾ä¸º-1çš„æ— æ•ˆæ•°æ®ï¼ˆKaggleä¸å‚ä¸è¯„åˆ†çš„æ ·æœ¬ï¼‰
        test_data = test_data[test_data['toxic'] != -1]
        print(f"-> æ¸…æ´—å®Œæˆã€‚æœ‰æ•ˆæµ‹è¯•æ ·æœ¬æ•°: {len(test_data)} (å‰”é™¤äº†æ ‡è®°ä¸º-1çš„æ•°æ®)")

        # æå–æ–‡æœ¬å’Œæ ‡ç­¾ï¼ˆå¡«å……ç©ºå€¼é˜²æ­¢æŠ¥é”™ï¼‰
        test_texts = test_data['comment_text'].fillna("").values
        test_labels = test_data[self.label_cols].values
        
        return test_data['id'].values, test_texts, test_labels

    def build_tokenizer(self):
        """æ„å»ºTokenizerï¼ˆåŸºäºè®­ç»ƒé›†ï¼Œä¿è¯è¯IDä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰"""
        print("-> æ­£åœ¨åˆ©ç”¨è®­ç»ƒé›†é‡å»ºè¯è¡¨ (Tokenizer)...")
        train_path = self.config['data']['train_path']
        if not os.path.exists(train_path):
            raise FileNotFoundError("é”™è¯¯ï¼šæ‰¾ä¸åˆ°è®­ç»ƒé›†ï¼Œæ— æ³•æ„å»ºè¯è¡¨ã€‚")
        
        train_df = pd.read_csv(train_path)
        train_texts = train_df['comment_text'].fillna("").values
        
        # åˆå§‹åŒ–å¹¶æ„å»ºè¯è¡¨
        tokenizer = TextTokenizer(
            max_len=self.config['data']['max_len'],
            min_freq=self.config['data']['min_freq']
        )
        tokenizer.build_vocab(train_texts)
        print(f"-> è¯è¡¨æ„å»ºå®Œæˆï¼Œè¯æ±‡é‡: {tokenizer.vocab_size}")
        return tokenizer

    def load_model(self, tokenizer):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print("-> æ­£åœ¨åŠ è½½æ¨¡å‹æƒé‡...")
        model_path = os.path.join(self.config['train']['model_save_dir'], "best_model.pth")
        if not os.path.exists(model_path):
            raise FileNotFoundError("é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ best_model.pthï¼Œè¯·å…ˆè¿è¡Œ train.py")

        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆå‚æ•°ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        model = LSTMClassifier(
            vocab_size=tokenizer.vocab_size,
            embed_dim=self.config['model']['embed_dim'],
            hidden_dim=self.config['model']['hidden_dim'],
            num_layers=self.config['model']['num_layers'],
            bidirectional=self.config['model']['bidirectional'],
            num_classes=self.config['model']['num_classes'],
            dropout=self.config['model']['dropout']
        )
        
        # åŠ è½½æƒé‡å¹¶ç§»è‡³è®¾å¤‡
        model.load_state_dict(torch.load(model_path, map_location=self.device))
        model.to(self.device)
        model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ï¼ˆç¦ç”¨Dropoutï¼‰
        return model

    def predict(self, model, test_loader):
        """æ¨¡å‹æ¨ç†ï¼ˆæ‰¹é‡é¢„æµ‹ï¼Œé¿å…å†…å­˜æº¢å‡ºï¼‰"""
        print("-> å¼€å§‹æ¨ç†...")
        all_preds = []
        all_labels = []

        # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒåŠ é€Ÿæ¨ç†
        with torch.no_grad():
            for batch in tqdm(test_loader, desc="Testing"):
                inputs = batch['input_ids'].to(self.device)
                labels = batch['label'].to(self.device)
                
                # æ¨¡å‹è¾“å‡ºï¼ˆlogitsï¼Œæœªç»è¿‡sigmoidï¼‰
                outputs = model(inputs)  # Shape: [batch_size, 6]
                
                # ä¿å­˜ç»“æœï¼ˆç§»è‡³CPUå¹¶è½¬ä¸ºnumpyï¼‰
                all_preds.append(outputs.cpu().numpy())
                all_labels.append(labels.cpu().numpy())

        # æ‹¼æ¥æ‰€æœ‰æ‰¹æ¬¡ç»“æœ
        all_preds = np.vstack(all_preds)
        all_labels = np.vstack(all_labels)
        return all_preds, all_labels

    def calculate_metrics(self, all_labels, all_preds):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼ˆAUCã€å‡†ç¡®ç‡ã€Macro F1ï¼‰"""
        metrics = {}
        print("\n" + "="*30)
        print("       æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°æŠ¥å‘Š       ")
        print("="*30)
        
        # 1. ROC-AUC Scoreï¼ˆå¤šæ ‡ç­¾ä»»åŠ¡æ ¸å¿ƒæŒ‡æ ‡ï¼‰
        try:
            auc_score = roc_auc_score(all_labels, all_preds, average='macro')
            metrics['auc'] = auc_score
            print(f"ğŸ† ROC-AUC Score: {auc_score:.5f}")
        except ValueError:
            metrics['auc'] = None
            print("âš ï¸ è­¦å‘Š: æ— æ³•è®¡ç®— ROC-AUC (å¯èƒ½æ˜¯æŸä¸ªç±»åˆ«çš„æ ‡ç­¾å…¨ä¸º0)")

        # 2. å…¨å±€å‡†ç¡®ç‡ï¼ˆç¡¬åˆ†ç±»ï¼šæ¦‚ç‡>0.5è§†ä¸º1ï¼‰
        hard_preds = (all_preds > 0.5).astype(int)
        acc = accuracy_score(all_labels.flatten(), hard_preds.flatten())
        metrics['accuracy'] = acc
        print(f"ğŸ“Š Global Accuracy: {acc:.5f}")

        # 3. Macro F1 Scoreï¼ˆå¤šæ ‡ç­¾ä»»åŠ¡æ ¸å¿ƒæŒ‡æ ‡ï¼Œå¹³è¡¡æ­£è´Ÿæ ·æœ¬ï¼‰
        f1 = f1_score(all_labels.flatten(), hard_preds.flatten(), average='macro')
        metrics['macro_f1'] = f1
        print(f"ğŸ“‰ Macro F1 Score : {f1:.5f}")
        
        print("-" * 30)
        return metrics

    def save_predictions(self, ids, all_preds):
        """ä¿å­˜é¢„æµ‹ç»“æœåˆ°CSVæ–‡ä»¶"""
        # æ„å»ºç»“æœDataFrame
        submission_df = pd.DataFrame({'id': ids})
        for i, col in enumerate(self.label_cols):
            submission_df[col] = all_preds[:, i]
        
        # ä¿å­˜è·¯å¾„ï¼ˆä»configè¯»å–ï¼Œç¡®ä¿ç›®å½•å­˜åœ¨ï¼‰
        save_dir = os.path.dirname(self.config['train']['result_save_dir'])
        os.makedirs(save_dir, exist_ok=True)
        save_file = os.path.join(save_dir, "test_predictions.csv")
        
        submission_df.to_csv(save_file, index=False)
        print(f"âœ… è¯¦ç»†é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: {save_file}")
        return save_file

    def run(self):
        """æ‰§è¡Œå®Œæ•´è¯„ä¼°æµç¨‹ï¼ˆä¸»å…¥å£ï¼‰"""
        try:
            # æ„å»ºTokenizer
            tokenizer = self.build_tokenizer()
            
            # åŠ è½½å¹¶æ¸…æ´—æµ‹è¯•é›†æ•°æ®
            test_ids, test_texts, test_labels = self.load_and_clean_test_data()
            
            # æ„å»ºæµ‹è¯•é›†DataLoader
            test_dataset = ToxicDataset(test_texts, test_labels, tokenizer)
            test_loader = DataLoader(
                test_dataset,
                batch_size=self.config['train']['batch_size'],
                shuffle=False,
                num_workers=0  
            )
            
            # åŠ è½½æ¨¡å‹
            model = self.load_model(tokenizer)
            
            # æ‰¹é‡é¢„æµ‹
            all_preds, all_labels = self.predict(model, test_loader)
            
            # è®¡ç®—æŒ‡æ ‡
            metrics = self.calculate_metrics(all_labels, all_preds)
            
            # ä¿å­˜ç»“æœ
            self.save_predictions(test_ids, all_preds)
            
            print("\n=== è¯„ä¼°å®Œæˆ ===")
            return metrics
        
        except Exception as e:
            print(f"\nâŒ è¯„ä¼°å¤±è´¥ï¼š{str(e)}")
            raise  # æŠ›å‡ºå¼‚å¸¸ï¼Œæ–¹ä¾¿è°ƒè¯•