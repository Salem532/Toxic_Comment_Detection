import torch
import argparse
import os
import sys
from src.utils.model_utils import load_config, seed_everything
from src.models.lstm import LSTMClassifier
from src.data.tokenizer import TextTokenizer

def predict_cli():
    # å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description="æ¶æ„è¯„è®ºæ£€æµ‹ - å•å¥é¢„æµ‹")
    parser.add_argument("--text", type=str, required=True, help="è¾“å…¥è¦æ£€æµ‹çš„è‹±æ–‡å¥å­")
    parser.add_argument("--config", type=str, default="configs/config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    args = parser.parse_args()

    # åŠ è½½é…ç½®
    if not os.path.exists(args.config):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶ {args.config}")
        return
    config = load_config(args.config)
    device = torch.device(config['train']['device'] if torch.cuda.is_available() else "cpu")

    # åŠ è½½ Tokenizer (å…³é”®æ­¥éª¤)
    tokenizer_path = os.path.join(config['train']['model_save_dir'], "tokenizer.pkl")
    if not os.path.exists(tokenizer_path):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° Tokenizer æ–‡ä»¶: {tokenizer_path}")
        print("   è¯·å…ˆè¿è¡Œ 'python train.py' æ¥ç”Ÿæˆå¹¶ä¿å­˜ Tokenizerã€‚")
        return
    
    try:
        tokenizer = TextTokenizer.load(tokenizer_path)
        print(f"âœ… æˆåŠŸåŠ è½½ Tokenizer (Vocab size: {tokenizer.vocab_size})")
    except Exception as e:
        print(f"âŒ Tokenizer åŠ è½½å¤±è´¥: {e}")
        return

    # åŠ è½½æ¨¡å‹
    model_path = os.path.join(config['train']['model_save_dir'], "best_model.pth")
    if not os.path.exists(model_path):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æƒé‡: {model_path}")
        return

    # è¿™é‡Œçš„å‚æ•°å¿…é¡»å’Œ config.yaml ä¸­è®­ç»ƒæ—¶çš„ä¸€è‡´
    model = LSTMClassifier(
        vocab_size=tokenizer.vocab_size,
        embed_dim=config['model']['embed_dim'],
        hidden_dim=config['model']['hidden_dim'],
        num_layers=config['model']['num_layers'],
        bidirectional=config['model']['bidirectional'],
        num_classes=config['model']['num_classes'],
        dropout=config['model']['dropout']
    )
    
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()

    # 5. å¤„ç†è¾“å…¥å¹¶é¢„æµ‹
    text = args.text
    input_ids = tokenizer.convert_tokens_to_ids(text)
    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device) # [1, max_len]

    print("\n" + "="*40)
    print(f"ğŸ“ è¾“å…¥æ–‡æœ¬: \"{text}\"")
    print("="*40)

    with torch.no_grad():
        # è¾“å‡ºæ˜¯æ¦‚ç‡å€¼ (0~1)
        probs = model(input_tensor).squeeze().cpu().numpy()

    # 6. æ ¼å¼åŒ–è¾“å‡ºç»“æœ
    labels = ["Toxic (æ¶æ„)", "Severe Toxic (ä¸¥é‡æ¶æ„)", "Obscene (æ·«ç§½)", 
              "Threat (å¨èƒ)", "Insult (ä¾®è¾±)", "Identity Hate (èº«ä»½ä»‡æ¨)"]
    
    print("ğŸ“Š é¢„æµ‹ç»“æœ:")
    print("-" * 40)
    for label, prob in zip(labels, probs):
        # ç®€å•çš„è¿›åº¦æ¡å¯è§†åŒ–
        bar_len = 20
        filled_len = int(bar_len * prob)
        bar = 'â–ˆ' * filled_len + 'â–‘' * (bar_len - filled_len)
        
        # æ¦‚ç‡ > 50% æ ‡çº¢ (å¦‚æœç»ˆç«¯æ”¯æŒ)
        prob_percent = prob * 100
        status = "âš ï¸ æ£€å‡º" if prob > 0.5 else "âœ… å®‰å…¨"
        
        print(f"{label:<25} | {bar} | {prob_percent:5.1f}% | {status}")
    print("-" * 40)

    # ç»¼åˆåˆ¤æ–­
    if any(probs > 0.5):
        print("\nğŸš« ç»“è®º: è¿™æ¡è¯„è®ºåŒ…å«æ¶æ„å†…å®¹ï¼")
    else:
        print("\nâœ¨ ç»“è®º: è¿™æ˜¯ä¸€æ¡å‹å–„çš„è¯„è®ºã€‚")

if __name__ == "__main__":
    predict_cli()